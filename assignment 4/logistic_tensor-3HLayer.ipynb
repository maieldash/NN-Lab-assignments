{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import seaborn as sns\n",
    "sns.set(style='whitegrid')\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = h5py.File('E:/FCI/year 4/second term/NN/labs/neural-network-and-deep-learning-master/lab3/train_catvnoncat.hdf5', \"r\")\n",
    "train_set_x = np.array(train_dataset[\"train_set_x\"][:]).astype(np.float32) # your train set features\n",
    "train_set_y = np.array(train_dataset[\"train_set_y\"][:]).astype(np.float32) # your train set labels\n",
    "\n",
    "test_dataset = h5py.File('E:/FCI/year 4/second term/NN/labs/neural-network-and-deep-learning-master/lab3/test_catvnoncat.hdf5', \"r\")\n",
    "test_set_x= np.array(test_dataset[\"test_set_x\"][:]).astype(np.float32) # your test set features\n",
    "test_set_y = np.array(test_dataset[\"test_set_y\"][:]).astype(np.float32) # your test set labels\n",
    "classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
    "data_size = train_set_x.shape[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set x dim= (209, 64, 64, 3)\n",
      "train set y dim= (209,)\n",
      "test set x dim= (50, 64, 64, 3)\n",
      "test set y dim= (50,)\n",
      "classes = (2,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#print dataset shapes(dimensions)\n",
    "print(\"train set x dim= \"+str(train_set_x.shape))\n",
    "print(\"train set y dim= \"+str(train_set_y.shape))\n",
    "print(\"test set x dim= \"+str(test_set_x.shape))\n",
    "print(\"test set y dim= \"+str(test_set_y.shape))\n",
    "print(\"classes = \"+str(classes.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 2)\n",
      "(209, 2)\n"
     ]
    }
   ],
   "source": [
    "#reshape dataset\n",
    "#train data\n",
    "train_set_x = train_set_x.reshape(train_set_x.shape[0],-1)\n",
    "train_set_y = train_set_y.reshape(1,train_set_y.shape[0]).T\n",
    "    \n",
    "#test data\n",
    "test_set_x = test_set_x.reshape(test_set_x.shape[0],-1)\n",
    "test_set_y = test_set_y.reshape(1,test_set_y.shape[0]).T\n",
    "\n",
    "train_set_y_n = np.where(train_set_y == 0, 1, 0)\n",
    "train_set_y =  np.c_[train_set_y,train_set_y_n]\n",
    "\n",
    "test_set_y_n = np.where(test_set_y == 0, 1, 0)\n",
    "test_set_y =  np.c_[test_set_y,test_set_y_n]\n",
    "print(test_set_y.shape)\n",
    "print(train_set_y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set x dim= (209, 12288)\n",
      "train set y dim= (209, 2)\n",
      "test set x dim= (50, 12288)\n",
      "test set y dim= (50, 2)\n",
      "data_size = 209\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#print dataset shapes(dimensions)\n",
    "print(\"train set x dim= \"+str(train_set_x.shape))\n",
    "print(\"train set y dim= \"+str(train_set_y.shape))\n",
    "print(\"test set x dim= \"+str(test_set_x.shape))\n",
    "print(\"test set y dim= \"+str(test_set_y.shape))\n",
    "print(\"data_size = \"+str(data_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize data\n",
    "train_set_x=train_set_x/255\n",
    "test_set_x=test_set_x/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 25\n",
    "batch_size = 209\n",
    "display_step = 1\n",
    "num_nodes_hl1=100\n",
    "num_nodes_hl2=100\n",
    "num_nodes_hl3=100\n",
    "num_classes =classes.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_model(data):\n",
    "    \n",
    "    \n",
    "    hidden_layer_1={'weights':tf.Variable(tf.random_normal([train_set_x.shape[1],num_nodes_hl1])),\n",
    "         'biases':tf.Variable(tf.random_normal([num_nodes_hl1]))}\n",
    "    \n",
    "    hidden_layer_2={'weights':tf.Variable(tf.random_normal([num_nodes_hl1,num_nodes_hl2])),\n",
    "         'biases':tf.Variable(tf.random_normal([num_nodes_hl2]))}\n",
    "    \n",
    "    hidden_layer_3={'weights':tf.Variable(tf.random_normal([num_nodes_hl2,num_nodes_hl3])),\n",
    "         'biases':tf.Variable(tf.random_normal([num_nodes_hl3]))}\n",
    "    \n",
    "    output_layer={'weights':tf.Variable(tf.random_normal([num_nodes_hl3,train_set_y.shape[1]])),\n",
    "         'biases':tf.Variable(tf.random_normal([train_set_y.shape[1]]))}\n",
    "\n",
    "    \n",
    "    l1= tf.add(tf.matmul(data, hidden_layer_1['weights']) ,hidden_layer_1['biases'])\n",
    "    l1= tf.nn.sigmoid(l1)\n",
    "    \n",
    "    l2= tf.add(tf.matmul(l1, hidden_layer_2['weights']) ,hidden_layer_2['biases'])\n",
    "    l2= tf.nn.sigmoid(l2)\n",
    "    \n",
    "    l3= tf.add(tf.matmul(l2, hidden_layer_3['weights']) ,hidden_layer_3['biases'])\n",
    "    l3= tf.nn.sigmoid(l3)\n",
    "    \n",
    "    output= tf.add(tf.matmul(l3, output_layer['weights']) ,output_layer['biases'])\n",
    "    output= tf.nn.sigmoid(output)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel():\n",
    "    x = tf.placeholder(tf.float32, [None, train_set_x.shape[1]])\n",
    "    y_ = tf.placeholder(tf.float32, [None, train_set_y.shape[1]])\n",
    "\n",
    "    #y_ = np.array([None, train_set_y.shape[1]]).astype(np.float32) # your train set labels\n",
    "    prediction=NN_model(train_set_x)\n",
    "     # calculating cost\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=train_set_y, logits=prediction)\n",
    "    cost=tf.reduce_mean(cross_entropy)\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    #train_step = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        # Training cycle\n",
    "        for epoch in range(training_epochs):\n",
    "            loss = 0.\n",
    "            total_batch = int(data_size/batch_size)\n",
    "            # Loop over all batches\n",
    "            for i in range(total_batch):\n",
    "                pos = (i % int(data_size / batch_size)) * batch_size\n",
    "                batch_xs = train_set_x[pos:pos+batch_size]\n",
    "                batch_ys = train_set_y[pos:pos+batch_size]\n",
    "                # Run optimization op (backprop) and cost op (to get loss value)\n",
    "\n",
    "                _,c = sess.run([train_step,cost], feed_dict={x: batch_xs, y_: batch_ys})\n",
    "                loss +=c\n",
    "            print('epoch',epoch,'out of',training_epochs,'loss :',loss)\n",
    "            \n",
    "        correct_prediction = tf.equal(tf.argmax(train_set_y, 1), tf.argmax(prediction, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        print('train accuracy %g' % accuracy.eval(feed_dict={x: train_set_x, y_: train_set_y}))\n",
    "        print('test accuracy %g' % accuracy.eval(feed_dict={x: test_set_x, y_: test_set_y}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 out of 25 loss : 0.6972510814666748\n",
      "epoch 1 out of 25 loss : 0.6629326343536377\n",
      "epoch 2 out of 25 loss : 0.6303824782371521\n",
      "epoch 3 out of 25 loss : 0.6299213767051697\n",
      "epoch 4 out of 25 loss : 0.6348095536231995\n",
      "epoch 5 out of 25 loss : 0.6260514259338379\n",
      "epoch 6 out of 25 loss : 0.600426435470581\n",
      "epoch 7 out of 25 loss : 0.6022652387619019\n",
      "epoch 8 out of 25 loss : 0.6092129349708557\n",
      "epoch 9 out of 25 loss : 0.6020423769950867\n",
      "epoch 10 out of 25 loss : 0.5944924354553223\n",
      "epoch 11 out of 25 loss : 0.5890531539916992\n",
      "epoch 12 out of 25 loss : 0.5847591757774353\n",
      "epoch 13 out of 25 loss : 0.5797384977340698\n",
      "epoch 14 out of 25 loss : 0.5827353596687317\n",
      "epoch 15 out of 25 loss : 0.5875601768493652\n",
      "epoch 16 out of 25 loss : 0.5827950835227966\n",
      "epoch 17 out of 25 loss : 0.5769856572151184\n",
      "epoch 18 out of 25 loss : 0.5782861709594727\n",
      "epoch 19 out of 25 loss : 0.5681508779525757\n",
      "epoch 20 out of 25 loss : 0.5672446489334106\n",
      "epoch 21 out of 25 loss : 0.5700365304946899\n",
      "epoch 22 out of 25 loss : 0.5749920010566711\n",
      "epoch 23 out of 25 loss : 0.5580005049705505\n",
      "epoch 24 out of 25 loss : 0.5865296125411987\n",
      "train accuracy 0.655502\n",
      "test accuracy 0.655502\n"
     ]
    }
   ],
   "source": [
    "trainModel()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
