{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets, svm, cross_validation, tree, preprocessing, metrics\n",
    "import h5py\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    train_dataset = h5py.File('E:/FCI/year 4/second term/NN/labs/neural-network-and-deep-learning-master/lab3/train_catvnoncat.hdf5', \"r\")\n",
    "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
    "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
    "\n",
    "    test_dataset = h5py.File('E:/FCI/year 4/second term/NN/labs/neural-network-and-deep-learning-master/lab3/test_catvnoncat.hdf5', \"r\")\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
    "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
    "\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
    "    \n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes=load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set x dim= (209, 64, 64, 3)\n",
      "train set y dim= (209,)\n",
      "test set x dim= (50, 64, 64, 3)\n",
      "test set y dim= (50,)\n"
     ]
    }
   ],
   "source": [
    "#print dataset shapes(dimensions)\n",
    "print(\"train set x dim= \"+str(train_set_x_orig.shape))\n",
    "print(\"train set y dim= \"+str(train_set_y_orig.shape))\n",
    "print(\"test set x dim= \"+str(test_set_x_orig.shape))\n",
    "print(\"test set y dim= \"+str(test_set_y_orig.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshape dataset\n",
    "#train data\n",
    "train_set_x = train_set_x_orig.reshape(train_set_x_orig.shape[0],-1).T\n",
    "train_set_y = train_set_y_orig.reshape(1,train_set_y_orig.shape[0])\n",
    "    \n",
    "#test data\n",
    "test_set_x = test_set_x_orig.reshape(test_set_x_orig.shape[0],-1).T\n",
    "test_set_y = test_set_y_orig.reshape(1,test_set_y_orig.shape[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set x dim= (12288, 209)\n",
      "train set y dim= (1, 209)\n",
      "test set x dim= (12288, 50)\n",
      "test set y dim= (1, 50)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#print dataset shapes(dimensions)\n",
    "print(\"train set x dim= \"+str(train_set_x.shape))\n",
    "print(\"train set y dim= \"+str(train_set_y.shape))\n",
    "print(\"test set x dim= \"+str(test_set_x.shape))\n",
    "print(\"test set y dim= \"+str(test_set_y.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize data\n",
    "train_set_x=train_set_x/255\n",
    "test_set_x=test_set_x/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(input):\n",
    "    \n",
    "    s = 1/(1 + np.exp(-input))\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layers_size(x,y):\n",
    "    n_x=x.shape[0]\n",
    "    n_y=y.shape[0]\n",
    "    return n_x,n_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialzation(n_x,n_h,n_y):\n",
    "\n",
    "    w1 = np.random.randn(n_h,n_x)\n",
    "    b1 = np.zeros((n_h,1))\n",
    "    w2 = np.random.randn(n_y,n_h)\n",
    "    b2 = np.zeros((n_y,1))\n",
    "              \n",
    "    parameters ={\"w1\": w1,\n",
    "                \"b1\": b1,\n",
    "                \"w2\": w2,\n",
    "                \"b2\": b2}                    \n",
    "                         \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardPropagation(X, parameters):\n",
    "   \n",
    "    \n",
    "    \n",
    "    w1 = parameters[\"w1\"]\n",
    "    w2 = parameters[\"w2\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    \n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # FORWARD PROPAGATION \n",
    "    z1 = np.dot(w1, X) + b1\n",
    "    \n",
    "    A1 = np.tanh(z1) \n",
    "    \n",
    "    z2 = np.dot(w2, A1) + b2\n",
    "    \n",
    "    A2 =sigmoid(z2) \n",
    "                \n",
    "                \n",
    "    cache ={    \"z1\": z1,\n",
    "                \"A1\": A1,\n",
    "                \"z2\": z2,\n",
    "                \"A2\": A2} \n",
    "    return cache \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost (A2,X,Y):\n",
    "    m  =  X.shape[1]\n",
    "    logs= np.multiply(np.log(A2) ,Y) + np.multiply((1-Y),np.log(1-A2))\n",
    "    cost= 1/m * np.sum(logs)\n",
    "    cost=np.squeeze(cost)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backwardPropagation(X,Y,cache, parameters):\n",
    "\n",
    "        \n",
    "    m  =  X.shape[1]\n",
    "    w1 = parameters[\"w1\"]\n",
    "    w2 = parameters[\"w2\"]\n",
    "    A1 = cache[\"A1\"]\n",
    "    A2 = cache[\"A2\"]\n",
    "    z1 = cache[\"z1\"]\n",
    "\n",
    "    # BACKWARD PROPAGATION (TO FIND GRAD)\n",
    "    dz2 = A2 - Y\n",
    "    \n",
    "    dw2 = 1/m * np.dot(dz2, A1.T)\n",
    "    \n",
    "    db2= 1/m * np.sum(dz2 , axis=1 , keepdims=True)\n",
    "    \n",
    "    dz1 = np.multiply(np.dot(w2.T,dz2) , (1-np.power(A1,2)))\n",
    "    \n",
    "    dw1 = 1/m * np.dot(dz1, X.T)\n",
    "    \n",
    "    db1 = 1/m * np.sum(dz1, axis=1 , keepdims=True)\n",
    "    \n",
    "    \n",
    "    grads = {\"dw1\": dw1,\n",
    "             \"db1\": db1,\n",
    "             \"dw2\": dw2,\n",
    "             \"db2\": db2}\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters,grads,learning_rate=1.2):\n",
    "    \n",
    "    w1 = parameters[\"w1\"]\n",
    "    w2 = parameters[\"w2\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    dw1 = grads[\"dw1\"]\n",
    "    db1 = grads[\"db1\"]\n",
    "    dw2 = grads[\"dw2\"]\n",
    "    db2 = grads[\"db2\"]\n",
    "\n",
    "    w1 = w1 - learning_rate * dw1\n",
    "    b1 = b1 - learning_rate * db1\n",
    "    w2 = w2 - learning_rate * dw2\n",
    "    b2 = b2 - learning_rate * db2\n",
    "     \n",
    "    parameters ={\"w1\": w1,\n",
    "                \"b1\": b1,\n",
    "                \"w2\": w2,\n",
    "                \"b2\": b2}                    \n",
    "                         \n",
    "    return parameters        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(parameters, X):\n",
    "\n",
    "    w1 = parameters[\"w1\"]\n",
    "    w2 = parameters[\"w2\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    b2 = parameters[\"b2\"] \n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1,m))\n",
    "    \n",
    "    \n",
    "    ### make prediction\n",
    "    A1 = np.tanh(np.dot(w1, X) + b1)\n",
    "    \n",
    "    A2 = sigmoid(np.dot(w2, A1) + b2)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    for i in range(A2.shape[1]):        \n",
    "        # Convert probabilities A[0,i] to actual predictions p[0,i]\n",
    "        if A2[0,i] >= 0.5:\n",
    "            Y_prediction[0,i] = 1\n",
    "        else:\n",
    "            Y_prediction[0,i] = 0\n",
    "    \n",
    "    \n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test,n_h,num_iterations=10000,learning_rate=1.2,print_cost = True):\n",
    "    \n",
    "    n_x,n_y = layers_size(X_train,Y_train)\n",
    "\n",
    "    parameters=initialzation(n_x,n_h,n_y)\n",
    "    costs=[]\n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        cache = forwardPropagation(X_train,parameters)\n",
    "        \n",
    "        A2 = cache[\"A2\"]\n",
    "        cost  = compute_cost (A2,X_train,Y_train)\n",
    "        \n",
    "        grads = backwardPropagation(X_train,Y_train,cache, parameters)\n",
    "        \n",
    "        parameters = update_parameters(parameters,grads,learning_rate)\n",
    "        \n",
    "                # Record the costs\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        \n",
    "        # Print the cost every 100 training examples\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    # Predict test/train set examples \n",
    "    Y_prediction_test = predict(parameters, X_test)\n",
    "    Y_prediction_train = predict(parameters, X_train)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Print train/test accuracy\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "\n",
    "    \n",
    "    d = {\"costs\": costs,\n",
    "         \"Y_prediction_test\": Y_prediction_test, \n",
    "         \"Y_prediction_train\" : Y_prediction_train, \n",
    "         \"grads\" : grads,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: -1.205442\n",
      "Cost after iteration 100: -0.610390\n",
      "Cost after iteration 200: -0.595413\n",
      "Cost after iteration 300: -0.589691\n",
      "Cost after iteration 400: -0.589327\n",
      "Cost after iteration 500: -0.580010\n",
      "Cost after iteration 600: -0.569152\n",
      "Cost after iteration 700: -0.563038\n",
      "Cost after iteration 800: -0.561700\n",
      "Cost after iteration 900: -0.607573\n",
      "Cost after iteration 1000: -0.543693\n",
      "Cost after iteration 1100: -0.539725\n",
      "Cost after iteration 1200: -0.522666\n",
      "Cost after iteration 1300: -0.598473\n",
      "Cost after iteration 1400: -0.598350\n",
      "Cost after iteration 1500: -0.594451\n",
      "Cost after iteration 1600: -0.589685\n",
      "Cost after iteration 1700: -0.529758\n",
      "Cost after iteration 1800: -0.474229\n",
      "Cost after iteration 1900: -0.493124\n",
      "train accuracy: 76.07655502392345 %\n",
      "test accuracy: 54.0 %\n"
     ]
    }
   ],
   "source": [
    "d = model(train_set_x, train_set_y, test_set_x, test_set_y,4, num_iterations = 2000, learning_rate = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
