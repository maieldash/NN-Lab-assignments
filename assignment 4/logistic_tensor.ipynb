{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import seaborn as sns\n",
    "sns.set(style='whitegrid')\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = h5py.File('E:/FCI/year 4/second term/NN/labs/neural-network-and-deep-learning-master/lab3/train_catvnoncat.hdf5', \"r\")\n",
    "train_set_x = np.array(train_dataset[\"train_set_x\"][:]).astype(np.float32) # your train set features\n",
    "train_set_y = np.array(train_dataset[\"train_set_y\"][:]).astype(np.float32) # your train set labels\n",
    "\n",
    "test_dataset = h5py.File('E:/FCI/year 4/second term/NN/labs/neural-network-and-deep-learning-master/lab3/test_catvnoncat.hdf5', \"r\")\n",
    "test_set_x= np.array(test_dataset[\"test_set_x\"][:]).astype(np.float32) # your test set features\n",
    "test_set_y = np.array(test_dataset[\"test_set_y\"][:]).astype(np.float32) # your test set labels\n",
    "classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
    "data_size = train_set_x.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set x dim= (209, 64, 64, 3)\n",
      "train set y dim= (209,)\n",
      "test set x dim= (50, 64, 64, 3)\n",
      "test set y dim= (50,)\n",
      "classes = (2,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#print dataset shapes(dimensions)\n",
    "print(\"train set x dim= \"+str(train_set_x.shape))\n",
    "print(\"train set y dim= \"+str(train_set_y.shape))\n",
    "print(\"test set x dim= \"+str(test_set_x.shape))\n",
    "print(\"test set y dim= \"+str(test_set_y.shape))\n",
    "print(\"classes = \"+str(classes.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 2)\n",
      "(209, 2)\n"
     ]
    }
   ],
   "source": [
    "#reshape dataset\n",
    "#train data\n",
    "train_set_x = train_set_x.reshape(train_set_x.shape[0],-1)\n",
    "train_set_y = train_set_y.reshape(1,train_set_y.shape[0]).T\n",
    "    \n",
    "#test data\n",
    "test_set_x = test_set_x.reshape(test_set_x.shape[0],-1)\n",
    "test_set_y = test_set_y.reshape(1,test_set_y.shape[0]).T\n",
    "\n",
    "train_set_y_n = np.where(train_set_y == 0, 1, 0)\n",
    "train_set_y =  np.c_[train_set_y,train_set_y_n]\n",
    "\n",
    "test_set_y_n = np.where(test_set_y == 0, 1, 0)\n",
    "test_set_y =  np.c_[test_set_y,test_set_y_n]\n",
    "print(test_set_y.shape)\n",
    "print(train_set_y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set x dim= (209, 12288)\n",
      "train set y dim= (209, 2)\n",
      "test set x dim= (50, 12288)\n",
      "test set y dim= (50, 2)\n",
      "data_size = 209\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#print dataset shapes(dimensions)\n",
    "print(\"train set x dim= \"+str(train_set_x.shape))\n",
    "print(\"train set y dim= \"+str(train_set_y.shape))\n",
    "print(\"test set x dim= \"+str(test_set_x.shape))\n",
    "print(\"test set y dim= \"+str(test_set_y.shape))\n",
    "print(\"data_size = \"+str(data_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize data\n",
    "train_set_x=train_set_x/255\n",
    "test_set_x=test_set_x/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 25\n",
    "batch_size = 209\n",
    "display_step = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 out of 25 loss : 0.6600207686424255\n",
      "epoch 1 out of 25 loss : 0.6600058674812317\n",
      "epoch 2 out of 25 loss : 0.6599910259246826\n",
      "epoch 3 out of 25 loss : 0.6599761843681335\n",
      "epoch 4 out of 25 loss : 0.6599613428115845\n",
      "epoch 5 out of 25 loss : 0.6599465012550354\n",
      "epoch 6 out of 25 loss : 0.6599317193031311\n",
      "epoch 7 out of 25 loss : 0.6599169969558716\n",
      "epoch 8 out of 25 loss : 0.6599022746086121\n",
      "epoch 9 out of 25 loss : 0.6598875522613525\n",
      "epoch 10 out of 25 loss : 0.6598728895187378\n",
      "epoch 11 out of 25 loss : 0.6598582863807678\n",
      "epoch 12 out of 25 loss : 0.6598438024520874\n",
      "epoch 13 out of 25 loss : 0.6598291993141174\n",
      "epoch 14 out of 25 loss : 0.659814715385437\n",
      "epoch 15 out of 25 loss : 0.6598002314567566\n",
      "epoch 16 out of 25 loss : 0.6597858667373657\n",
      "epoch 17 out of 25 loss : 0.6597715616226196\n",
      "epoch 18 out of 25 loss : 0.6597572565078735\n",
      "epoch 19 out of 25 loss : 0.6597430109977722\n",
      "epoch 20 out of 25 loss : 0.6597288846969604\n",
      "epoch 21 out of 25 loss : 0.6597146987915039\n",
      "epoch 22 out of 25 loss : 0.6597006916999817\n",
      "epoch 23 out of 25 loss : 0.6596866846084595\n",
      "epoch 24 out of 25 loss : 0.659672737121582\n",
      "Optimization Finished!\n",
      "train accuracy 0.650718\n",
      "test accuracy 0.34\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# for visualize purpose in tensorboard we use tf.name_scope\n",
    "with tf.name_scope(\"Declaring_placeholder\"):\n",
    "    # X is placeholdre for iris features. We will feed data later on\n",
    "    x = tf.placeholder(tf.float32, [None, train_set_x.shape[1]])\n",
    "    # y is placeholder for iris labels. We will feed data later on\n",
    "    y_ = tf.placeholder(tf.float32, [None, train_set_y.shape[1]])\n",
    "\n",
    "\n",
    "with tf.name_scope(\"Declaring_variables\"):\n",
    "    \n",
    "    # W is our weights. This will update during training time\n",
    "    W = tf.Variable(tf.random_normal([train_set_x.shape[1],train_set_y.shape[1]]))\n",
    "    # b is our bias. This will also update during training time\n",
    "    b = tf.Variable(tf.random_normal([train_set_y.shape[1]]))\n",
    "\n",
    "with tf.name_scope(\"Declaring_functions\"):\n",
    "    # our prediction function\n",
    "    y = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "\n",
    "\n",
    "with tf.name_scope(\"calculating_cost\"):\n",
    "    # calculating cost\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_, logits=y)\n",
    "    cost=tf.reduce_mean(cross_entropy)\n",
    "\n",
    "\n",
    "\n",
    "with tf.name_scope(\"declaring_gradient_descent\"):\n",
    "    # optimizer\n",
    "    # we use gradient descent for our optimizer \n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "#####\n",
    "with tf.name_scope(\"starting_tensorflow_session\"):\n",
    "        # Start training\n",
    "        sess = tf.InteractiveSession()\n",
    "        # Initialize the variables (i.e. assign their default value)\n",
    "        init = tf.global_variables_initializer()\n",
    "        # Run the initializer\n",
    "        sess.run(init)\n",
    "\n",
    "        # Training cycle\n",
    "        for epoch in range(training_epochs):\n",
    "            loss = 0.\n",
    "            total_batch = int(data_size/batch_size)\n",
    "            # Loop over all batches\n",
    "            for i in range(total_batch):\n",
    "                pos = (i % int(data_size / batch_size)) * batch_size\n",
    "                batch_xs = train_set_x[pos:pos+batch_size]\n",
    "                batch_ys = train_set_y[pos:pos+batch_size]\n",
    "                # Run optimization op (backprop) and cost op (to get loss value)\n",
    "                _,c = sess.run([train_step,cost], feed_dict={x: batch_xs, y_: batch_ys})\n",
    "                loss +=c\n",
    "            print('epoch',epoch,'out of',training_epochs,'loss :',loss)\n",
    "        print(\"Optimization Finished!\")\n",
    "\n",
    "\n",
    "        # Calculate accuracy\n",
    "        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        print('train accuracy %g' % accuracy.eval(feed_dict={x: train_set_x, y_: train_set_y}))        \n",
    "        print('test accuracy %g' % accuracy.eval(feed_dict={x: test_set_x, y_: test_set_y}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
