{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets, svm, cross_validation, tree, preprocessing, metrics\n",
    "import h5py\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    train_dataset = h5py.File('E:/FCI/year 4/second term/NN/labs/neural-network-and-deep-learning-master/lab3/train_catvnoncat.hdf5', \"r\")\n",
    "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
    "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
    "\n",
    "    test_dataset = h5py.File('E:/FCI/year 4/second term/NN/labs/neural-network-and-deep-learning-master/lab3/test_catvnoncat.hdf5', \"r\")\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
    "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
    "\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
    "    \n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes=load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set x dim= (209, 64, 64, 3)\n",
      "train set y dim= (209,)\n",
      "test set x dim= (50, 64, 64, 3)\n",
      "test set y dim= (50,)\n"
     ]
    }
   ],
   "source": [
    "#print dataset shapes(dimensions)\n",
    "print(\"train set x dim= \"+str(train_set_x_orig.shape))\n",
    "print(\"train set y dim= \"+str(train_set_y_orig.shape))\n",
    "print(\"test set x dim= \"+str(test_set_x_orig.shape))\n",
    "print(\"test set y dim= \"+str(test_set_y_orig.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshape dataset\n",
    "#train data\n",
    "train_set_x = train_set_x_orig.reshape(train_set_x_orig.shape[0],-1).T\n",
    "train_set_y = train_set_y_orig.reshape(1,train_set_y_orig.shape[0])\n",
    "    \n",
    "#test data\n",
    "test_set_x = test_set_x_orig.reshape(test_set_x_orig.shape[0],-1).T\n",
    "test_set_y = test_set_y_orig.reshape(1,test_set_y_orig.shape[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set x dim= (12288, 209)\n",
      "train set y dim= (1, 209)\n",
      "test set x dim= (12288, 50)\n",
      "test set y dim= (1, 50)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#print dataset shapes(dimensions)\n",
    "print(\"train set x dim= \"+str(train_set_x.shape))\n",
    "print(\"train set y dim= \"+str(train_set_y.shape))\n",
    "print(\"test set x dim= \"+str(test_set_x.shape))\n",
    "print(\"test set y dim= \"+str(test_set_y.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize data\n",
    "train_set_x=train_set_x/255\n",
    "test_set_x=test_set_x/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(input):\n",
    "    \n",
    "    s = 1/(1 + np.exp(-input))\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialzation():\n",
    "    \n",
    "    l1_w = np.random.randn(num_hidden_nodes[0],train_set_x.shape[0])\n",
    "    l1_b= np.zeros((num_hidden_nodes[0],1))\n",
    "\n",
    "    l2_w = np.random.randn(num_hidden_nodes[1],num_hidden_nodes[0])\n",
    "    l2_b= np.zeros((num_hidden_nodes[1],1))\n",
    "       \n",
    "    l3_w = np.random.randn(num_hidden_nodes[2],num_hidden_nodes[1])\n",
    "    l3_b= np.zeros((num_hidden_nodes[2],1))\n",
    "    \n",
    "    l4_w = np.random.randn(num_hidden_nodes[3],num_hidden_nodes[2])\n",
    "    l4_b= np.zeros((num_hidden_nodes[3],1))\n",
    "    \n",
    "    lOut_w = np.random.randn(train_set_y.shape[0],num_hidden_nodes[3])\n",
    "    lOut_b = np.zeros((train_set_y.shape[0],1))\n",
    "    weights= np.array([l1_w,l2_w,l3_w,l4_w,lOut_w])\n",
    "    biases= np.array([l1_b,l2_b,l3_b,l4_b,lOut_b])\n",
    "    \n",
    "    print(l1_w.shape)\n",
    "    print(l1_b.shape)\n",
    "    print(l2_w.shape)\n",
    "    print(l2_b.shape)\n",
    "    print(l3_w.shape)\n",
    "    print(l3_b.shape)\n",
    "    print(l4_w.shape)\n",
    "    print(l4_b.shape)\n",
    "    print(lOut_w.shape)\n",
    "    print(lOut_b.shape)\n",
    "    print(weights.shape)\n",
    "    print(biases.shape)\n",
    "    return weights,biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardPropagation(X,weights,biases):\n",
    "    \n",
    "    \n",
    "    z1 = np.dot(weights[0],X) + biases[0]\n",
    "    A1 = np.tanh(z1)\n",
    "    z2= np.dot( weights[1],A1) + biases[1]\n",
    "    A2 = np.tanh(z2)\n",
    "    z3= np.dot( weights[2],A2) + biases[2]\n",
    "    A3 = np.tanh(z3)\n",
    "    z4= np.dot( weights[3],A3) + biases[3]\n",
    "    A4 = np.tanh(z4)\n",
    "    \n",
    "    Z= np.array([z1,z2,z3,z4])\n",
    "    A= np.array([A1,A2,A3,A4])\n",
    "\n",
    "    output= np.dot(weights[4],A4) + biases[4]\n",
    "    output= sigmoid(output)\n",
    "    cache ={\"A\":A,\n",
    "            \"Z\":Z,\n",
    "            \"output\": output} \n",
    "    \n",
    "    return cache\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost (output,X,Y):\n",
    "    m  =  X.shape[1]\n",
    "    logs= np.multiply(np.log(output) ,Y) + np.multiply((1-Y),np.log(1-output))\n",
    "    cost= 1/m * np.sum(logs)\n",
    "    cost=np.squeeze(cost)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def backwardPropagation(X,Y,cache):\n",
    "\n",
    "        \n",
    "    m  =  X.shape[1]\n",
    "    A = cache[\"A\"]\n",
    "    Z = cache[\"Z\"]\n",
    "    output = cache[\"output\"]\n",
    "\n",
    "    # BACKWARD PROPAGATION (TO FIND GRAD)\n",
    "    dzout = output - Y  \n",
    "    DW = 1/m * np.dot(dzout, A[A.shape[0]-1].T)\n",
    "    DB = 1/m * np.sum(dzout , axis=1 , keepdims=True)\n",
    "    \n",
    "    DW= np.array([DW])\n",
    "    DB= np.array([DB])   \n",
    "    print(DW.shape)\n",
    "    print(DB.shape)\n",
    "    dz4 = np.multiply(np.dot(weights[weights.shape[0]-1].T,dzout) , (1-np.power(A[A.shape[0]-1],2)))\n",
    "    dw4 = 1/m * np.dot(dz4,A[A.shape[0]-2].T)\n",
    "    db4 = 1/m * np.sum(dz4, axis=1 , keepdims=True)\n",
    "    print(dw4.shape)\n",
    "    print(db4.shape)\n",
    "    DW = np.append(DW, [dw4], axis=0)\n",
    "    DW = np.append(DB, [db4], axis=0)\n",
    "\n",
    "    zz=dz4\n",
    "    for i in range(2,A.shape[0]):\n",
    "        print(i)\n",
    "        dzi = np.multiply(np.dot(weights[weights.shape[0]-i].T,zz) , (1-np.power(A[A.shape[0]-i],2)))\n",
    "        dwi = 1/m * np.dot(dzi,A[A.shape[0]-i].T)\n",
    "        dbi = 1/m * np.sum(dzi, axis=1 , keepdims=True)\n",
    "        DW = np.append(DW, [dwi], axis=0)\n",
    "        DW = np.append(DB, [dbi], axis=0)\n",
    "        zz=dzi\n",
    "        \n",
    "    dz1 = np.multiply(np.dot(weights[weights.shape[0]-4].T,zz) , (1-np.power(A[A.shape[0]-4],2)))\n",
    "    dw1 = 1/m * np.dot(dz1,X.T)\n",
    "    db1 = 1/m * np.sum(dz1, axis=1 , keepdims=True)\n",
    "    DW = np.append(DW, [dw1], axis=0)\n",
    "    DW = np.append(DB, [db1], axis=0)    \n",
    "    \n",
    "    print(DW.shape)\n",
    "    print(DB.shape)\n",
    "    grads = {\"dW\": DW,\n",
    "             \"dB\": DB}\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backwardPropagation(X,Y,cache,weights,biases):\n",
    "\n",
    "        \n",
    "    m  =  X.shape[1]\n",
    "    A = cache[\"A\"]\n",
    "    Z = cache[\"Z\"]\n",
    "    output = cache[\"output\"]\n",
    "\n",
    "    # BACKWARD PROPAGATION (TO FIND GRAD)\n",
    "    dzout = output - Y  \n",
    "    dw_out = 1/m * np.dot(dzout, A[A.shape[0]-1].T)\n",
    "    db_out = 1/m * np.sum(dzout , axis=1 , keepdims=True)\n",
    "\n",
    "    dz4 = np.multiply(np.dot(weights[weights.shape[0]-1].T,dzout) , (1-np.power(A[A.shape[0]-1],2)))\n",
    "    dw4 = 1/m * np.dot(dz4,A[A.shape[0]-2].T)\n",
    "    db4 = 1/m * np.sum(dz4, axis=1 , keepdims=True)\n",
    "    \n",
    "    dz3 = np.multiply(np.dot(weights[weights.shape[0]-2].T,dz4) , (1-np.power(A[A.shape[0]-2],2)))\n",
    "    dw3 = 1/m * np.dot(dz3,A[A.shape[0]-3].T)\n",
    "    db3 = 1/m * np.sum(dz3, axis=1 , keepdims=True)\n",
    "    \n",
    "    dz2 = np.multiply(np.dot(weights[weights.shape[0]-3].T,dz3) , (1-np.power(A[A.shape[0]-3],2)))\n",
    "    dw2 = 1/m * np.dot(dz2,A[A.shape[0]-4].T)\n",
    "    db2 = 1/m * np.sum(dz2, axis=1 , keepdims=True)\n",
    "    \n",
    "    dz1 = np.multiply(np.dot(weights[weights.shape[0]-4].T,dz2) , (1-np.power(A[A.shape[0]-4],2)))\n",
    "    dw1 = 1/m * np.dot(dz1,X.T)\n",
    "    db1 = 1/m * np.sum(dz1, axis=1 , keepdims=True)\n",
    "    grads = {\"dw1\": dw1,\n",
    "             \"db1\": db1,\n",
    "            \"dw2\": dw2,\n",
    "            \"db2\": db2,\n",
    "            \"dw3\": dw3,\n",
    "            \"db3\": db3,\n",
    "            \"dw4\": dw4,\n",
    "            \"db4\": db4,\n",
    "            \"dw_out\": dw_out,\n",
    "            \"db_out\": db_out}\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(weights,biases,grads,learning_rate=1.2):\n",
    "    \n",
    "    \n",
    "    dw1 = grads[\"dw1\"]\n",
    "    db1 = grads[\"db1\"]\n",
    "    dw2 = grads[\"dw2\"]\n",
    "    db2 = grads[\"db2\"]\n",
    "    dw3 = grads[\"dw3\"]\n",
    "    db3 = grads[\"db3\"]\n",
    "    dw4 = grads[\"dw4\"]\n",
    "    db4 = grads[\"db4\"]\n",
    "    dw_out = grads[\"dw_out\"]\n",
    "    db_out = grads[\"db_out\"]\n",
    "    weights[0] = weights[0] - learning_rate * dw1\n",
    "    biases[0] = biases[0] - learning_rate * db1\n",
    "    weights[1] = weights[1] - learning_rate * dw2\n",
    "    biases[1] = biases[1] - learning_rate * db2\n",
    "    weights[2] = weights[2] - learning_rate * dw3\n",
    "    biases[2] = biases[2] - learning_rate * db3\n",
    "    weights[3] = weights[3] - learning_rate * dw4\n",
    "    biases[3] = biases[3] - learning_rate * db4\n",
    "    weights[4] = weights[4] - learning_rate * dw_out\n",
    "    biases[4] = biases[4] - learning_rate * db_out       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(weights,biases, X):\n",
    "\n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1,m))\n",
    "    \n",
    "    \n",
    "    ### make prediction\n",
    "    z1 = np.dot(weights[0],X) + biases[0]\n",
    "    A1 = np.tanh(z1)\n",
    "    z2= np.dot( weights[1],A1) + biases[1]\n",
    "    A2 = np.tanh(z2)\n",
    "    z3= np.dot( weights[2],A2) + biases[2]\n",
    "    A3 = np.tanh(z3)\n",
    "    z4= np.dot( weights[3],A3) + biases[3]\n",
    "    A4 = np.tanh(z4)\n",
    "    \n",
    "    Z= np.array([z1,z2,z3,z4])\n",
    "    A= np.array([A1,A2,A3,A4])\n",
    "\n",
    "    output= np.dot(weights[4],A4) + biases[4]\n",
    "    output= sigmoid(output)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    for i in range(output.shape[1]):        \n",
    "        if output[0,i] >= 0.5:\n",
    "            Y_prediction[0,i] = 1\n",
    "        else:\n",
    "            Y_prediction[0,i] = 0\n",
    "    \n",
    "    \n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_x=train_set_x.shape[0]\n",
    "num_hidden_nodes= np.array([80,40,20,10])\n",
    "n_y=train_set_x.shape[0]\n",
    "np.random.seed(1)\n",
    "print(num_hidden_nodes.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test,n_h,num_iterations=10000,learning_rate=1.2,print_cost = True):\n",
    "    \n",
    "    costs=[]\n",
    "    weights,biases= initialzation()\n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        cache = forwardPropagation(X_train,weights,biases)\n",
    "        \n",
    "        output = cache[\"output\"]\n",
    "        cost  = compute_cost (output,X_train,Y_train) \n",
    "        grads = backwardPropagation(X_train,Y_train,cache,weights,biases)\n",
    "        \n",
    "        update_parameters(weights,biases,grads,learning_rate)\n",
    "        \n",
    "                # Record the costs\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        \n",
    "        # Print the cost every 100 training examples\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    # Predict test/train set examples \n",
    "    Y_prediction_test = predict(weights,biases, X_test)\n",
    "    Y_prediction_train = predict(weights,biases,X_train)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Print train/test accuracy\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "\n",
    "    \n",
    "    d = {\"costs\": costs,\n",
    "         \"Y_prediction_test\": Y_prediction_test, \n",
    "         \"Y_prediction_train\" : Y_prediction_train, \n",
    "         \"grads\" : grads,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 12288)\n",
      "(80, 1)\n",
      "(40, 80)\n",
      "(40, 1)\n",
      "(20, 40)\n",
      "(20, 1)\n",
      "(10, 20)\n",
      "(10, 1)\n",
      "(1, 10)\n",
      "(1, 1)\n",
      "(5,)\n",
      "(5,)\n",
      "Cost after iteration 0: -2.126781\n",
      "Cost after iteration 100: -0.500367\n",
      "train accuracy: 81.33971291866028 %\n",
      "test accuracy: 36.0 %\n"
     ]
    }
   ],
   "source": [
    "d = model(train_set_x, train_set_y, test_set_x, test_set_y,4, num_iterations = 200, learning_rate = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
