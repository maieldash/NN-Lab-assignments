{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets, svm, cross_validation, tree, preprocessing, metrics\n",
    "import sklearn.ensemble as ske\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic= pd.read_csv(\"E:/FCI/year 4/second term/NN/labs/neural-network-and-deep-learning-master/lab3/titanic_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Survived  Pclass  \\\n",
      "0            1         0       3   \n",
      "1            2         1       1   \n",
      "2            3         1       3   \n",
      "3            4         1       1   \n",
      "4            5         0       3   \n",
      "\n",
      "                                                Name     Sex   Age  SibSp  \\\n",
      "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
      "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
      "4                           Allen, Mr. William Henry    male  35.0      0   \n",
      "\n",
      "   Parch            Ticket     Fare Cabin Embarked  \n",
      "0      0         A/5 21171   7.2500   NaN        S  \n",
      "1      0          PC 17599  71.2833   C85        C  \n",
      "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
      "3      0            113803  53.1000  C123        S  \n",
      "4      0            373450   8.0500   NaN        S  \n"
     ]
    }
   ],
   "source": [
    "print(titanic.head());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               PassengerId  Survived        Age     SibSp     Parch  \\\n",
      "Pclass Sex                                                            \n",
      "1      female   469.212766  0.968085  34.611765  0.553191  0.457447   \n",
      "       male     455.729508  0.368852  41.281386  0.311475  0.278689   \n",
      "2      female   443.105263  0.921053  28.722973  0.486842  0.605263   \n",
      "       male     447.962963  0.157407  30.740707  0.342593  0.222222   \n",
      "3      female   399.729167  0.500000  21.750000  0.895833  0.798611   \n",
      "       male     455.515850  0.135447  26.507589  0.498559  0.224784   \n",
      "\n",
      "                     Fare  \n",
      "Pclass Sex                 \n",
      "1      female  106.125798  \n",
      "       male     67.226127  \n",
      "2      female   21.970121  \n",
      "       male     19.741782  \n",
      "3      female   16.118810  \n",
      "       male     12.661633  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x11a54240>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x118abe10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sex_grouping = titanic.groupby(['Pclass','Sex']).mean()\n",
    "print(sex_grouping)\n",
    "sex_grouping['Survived'].plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the graph show us that :\n",
    "  -- females have been survived more than males\n",
    "  -- calss 1 survived more than class 2\n",
    "  -- class 2 survived more than class 3\n",
    " so we conclude that classes and Sex affect the learning processing \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic=titanic.drop(['Name','Embarked','Ticket','Cabin','PassengerId'],axis=1)\n",
    "print(titanic.head());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Name','Embarked','Ticket','Cabin','PassengerId' are not important to the learning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_by_age = pd.cut(titanic[\"Age\"], np.arange(0, 90, 10))\n",
    "age_grouping = titanic.groupby(group_by_age).mean()\n",
    "age_grouping['Survived'].plot.bar()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "he graph show us that :\n",
    "  -- ages between 0 and 10 have the heighst probaility and it decreases with older people \n",
    "  \n",
    " so we conclude that Age affect the learning processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(titanic.SibSp)\n",
    "group_by_sib = pd.cut(titanic[\"SibSp\"], np.arange(0, 10, 1))\n",
    "sib_grouping = titanic.groupby(group_by_sib).mean()\n",
    "sib_grouping['Survived'].plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(titanic.Parch)\n",
    "group_by_parch = pd.cut(titanic[\"Parch\"], np.arange(0, 10, 1))\n",
    "parch_grouping = titanic.groupby(group_by_parch).mean()\n",
    "parch_grouping['Survived'].plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic[ \"Age\" ] = titanic.Age.fillna( titanic.Age.mean())\n",
    "titanic.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(df):\n",
    "    processed_df = df.copy()\n",
    "    pd.Series(processed_df.Sex).astype('category').cat.codes.values\n",
    "    processed_df.Sex =pd.Series(processed_df.Sex).astype('category').cat.codes.values \n",
    "    return processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df = preprocessing(titanic)\n",
    "print(processed_df.Sex)\n",
    "print(titanic.Sex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitDataSet(df):\n",
    "    X = processed_df.drop(['Survived'], axis=1).values\n",
    "    Y = processed_df['Survived'].values\n",
    "    train_set_x_orig,test_set_x_orig ,train_set_y_orig,  test_set_y_orig = cross_validation.train_test_split(X,Y,test_size=0.2)\n",
    "    print(\"train set x dim= \"+str(train_set_x_orig.shape))\n",
    "    print(\"train set y dim= \"+str(train_set_y_orig.shape))\n",
    "    print(\"test set x dim= \"+str(test_set_x_orig.shape))\n",
    "    print(\"test set y dim= \"+str(test_set_y_orig.shape))\n",
    "    print(\"after transpose ...\")\n",
    "    print(\"train set x dim= \"+str(train_set_x_orig.T.shape))\n",
    "    print(\"train set y dim= \"+str(train_set_y_orig.shape))\n",
    "    print(\"test set x dim= \"+str(test_set_x_orig.T.shape))\n",
    "    print(\"test set y dim= \"+str(test_set_y_orig.shape))\n",
    "    return train_set_x_orig.T,test_set_x_orig.T ,train_set_y_orig,  test_set_y_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_x, test_set_x, train_set_y, test_set_y = splitDataSet(titanic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(input):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of input\n",
    "\n",
    "    Arguments:\n",
    "    input -- A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(input)\n",
    "    \"\"\"\n",
    "\n",
    "    s = 1/(1 + np.exp(-input))\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_with_zeros(dim):\n",
    "    \"\"\"\n",
    "    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n",
    "    \n",
    "    Argument:\n",
    "    dim -- number of parameters \n",
    "    \n",
    "    Returns:\n",
    "    w -- initialized vector of shape (dim, 1)\n",
    "    b -- initialized scalar (corresponds to the bias)\n",
    "    \"\"\"\n",
    "    \n",
    "    w = np.zeros((dim, 1))\n",
    "    b = 0.0\n",
    "    \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: propagate\n",
    "\n",
    "def propagate(w, b, X, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function and its gradient \n",
    "\n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_Critirea, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (num_Critirea, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n",
    "\n",
    "    Return:\n",
    "    cost -- negative log-likelihood cost for logistic regression\n",
    "    dw -- gradient of the loss with respect to w, thus same shape as w\n",
    "    db -- gradient of the loss with respect to b, thus same shape as b\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # FORWARD PROPAGATION \n",
    "\n",
    "    A = sigmoid(np.dot(w.T, X) + b) \n",
    "    print(Y.shape)\n",
    "    \n",
    "    \n",
    "    # compute activation\n",
    "    cost = - 1/m * np.sum((Y * np.log(A) + (1 - Y) * np.log(1 - A)))          # compute cost\n",
    "    \n",
    "    \n",
    "    # BACKWARD PROPAGATION (TO FIND GRAD)\n",
    "    \n",
    "    dw =  1/m * np.dot(X, (A - Y).T)\n",
    "    db = 1/m * np.sum(A-Y)\n",
    "    \n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return grads, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimize FUNCTION\n",
    "\n",
    "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
    "    \"\"\"\n",
    "    This function optimizes w and b by running a gradient descent algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_criterea, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of shape (num_criterea, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- True to print the loss every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing the weights w and bias b\n",
    "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "    \n",
    "    Notes:\n",
    "    there are two steps to iterate through them:\n",
    "        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n",
    "        2) Update the parameters using gradient descent rule for w and b.\n",
    "    \"\"\"\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        grads, cost = propagate(w, b, X, Y)\n",
    "        \n",
    "        \n",
    "        # Retrieve derivatives from grads\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "        \n",
    "        #updates\n",
    "        w = w - learning_rate * dw\n",
    "        b = b - learning_rate * db\n",
    "        \n",
    "        \n",
    "        # Record the costs\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        \n",
    "        # Print the cost every 100 training examples\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return params, grads, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w, b, X):\n",
    "    '''\n",
    "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_Criterea, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (num_Criterea, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
    "    '''\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1,m))\n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "    \n",
    "    \n",
    "    ### make prediction\n",
    "    A = sigmoid(np.dot(w.T, X) + b)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    for i in range(A.shape[1]):        \n",
    "        # Convert probabilities A[0,i] to actual predictions p[0,i]\n",
    "        if A[0,i] >= 0.5:\n",
    "            Y_prediction[0,i] = 1\n",
    "        else:\n",
    "            Y_prediction[0,i] = 0\n",
    "    \n",
    "    \n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n",
    "    \"\"\"\n",
    "    Builds the logistic regression model by calling the function you've implemented previously\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set represented by a numpy array of shape (num_Criterea, m_train)\n",
    "    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n",
    "    X_test -- test set represented by a numpy array of shape (num_Criterea, m_test)\n",
    "    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n",
    "    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
    "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
    "    print_cost -- Set to true to print the cost every 100 iterations\n",
    "    \n",
    "    Returns:\n",
    "    d -- dictionary containing information about the model.\n",
    "    \"\"\"\n",
    "    \n",
    "    #initialize\n",
    "    w, b = initialize_with_zeros(X_train.shape[0])  # where to get dimension?\n",
    "\n",
    "    # Gradient descent \n",
    "    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n",
    "    \n",
    "    # Retrieve parameters w and b from dictionary \"parameters\"\n",
    "    w = parameters[\"w\"]\n",
    "    b = parameters[\"b\"]\n",
    "    \n",
    "    # Predict test/train set examples \n",
    "    Y_prediction_test = predict(w, b, X_test)\n",
    "    Y_prediction_train = predict(w, b, X_train)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Print train/test accuracy\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "\n",
    "    \n",
    "    d = {\"costs\": costs,\n",
    "         \"Y_prediction_test\": Y_prediction_test, \n",
    "         \"Y_prediction_train\" : Y_prediction_train, \n",
    "         \"w\" : w, \n",
    "         \"b\" : b,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_set_y.shape)\n",
    "d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pclass Sex  Age  SibSp  Parch Fare\n",
    "test_EX=np.array([[3,1,21,2,1,21]],float)\n",
    "result = predict(d[\"w\"], d[\"b\"], test_EX.T)\n",
    "if result ==1:\n",
    "   print(\"survived\")\n",
    "else:\n",
    "   print(\"not survived\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_EX=np.array([[1,0,21,1,0,71]],float)\n",
    "result = predict(d[\"w\"], d[\"b\"], test_EX.T)\n",
    "if result ==1:\n",
    "   print(\"survived\")\n",
    "else:\n",
    "   print(\"not survived\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
